{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef9daa8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "501b158b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(\"GPUs available:\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7c061d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MobileFaceNet Block Definitions\n",
    "def conv_block(inputs, filters, kernel, strides, activation=True):\n",
    "    x = layers.Conv2D(filters, kernel, strides=strides, padding='same', use_bias=False)(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    if activation:\n",
    "        x = layers.PReLU(shared_axes=[1, 2])(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c28cf059",
   "metadata": {},
   "outputs": [],
   "source": [
    "def depthwise_conv_block(inputs, kernel, strides, activation=True):\n",
    "    x = layers.DepthwiseConv2D(kernel, strides=strides, padding='same', use_bias=False)(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    if activation:\n",
    "        x = layers.PReLU(shared_axes=[1, 2])(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5eca515",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bottleneck(inputs, out_channels, expansion, strides):\n",
    "    in_channels = inputs.shape[-1]\n",
    "    x = conv_block(inputs, in_channels * expansion, 1, 1)\n",
    "    x = depthwise_conv_block(x, 3, strides)\n",
    "    x = layers.Conv2D(out_channels, 1, strides=1, padding='same', use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    if strides == 1 and in_channels == out_channels:\n",
    "        x = layers.Add()([inputs, x])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e16d296",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MobileFaceNet(input_shape=(112, 112, 3), embedding_size=128, num_classes=None):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    x = conv_block(inputs, 64, 3, 2)\n",
    "    x = depthwise_conv_block(x, 3, 1)\n",
    "    x = bottleneck(x, 64, 2, 2)\n",
    "    x = bottleneck(x, 64, 2, 1)\n",
    "    x = bottleneck(x, 64, 2, 1)\n",
    "    x = bottleneck(x, 64, 2, 1)\n",
    "    x = bottleneck(x, 128, 4, 2)\n",
    "    x = bottleneck(x, 128, 2, 1)\n",
    "    x = bottleneck(x, 128, 2, 1)\n",
    "    x = bottleneck(x, 128, 2, 1)\n",
    "    x = bottleneck(x, 128, 2, 1)\n",
    "    x = bottleneck(x, 128, 2, 1)\n",
    "    x = bottleneck(x, 128, 2, 1)\n",
    "    x = bottleneck(x, 128, 2, 1)\n",
    "    x = conv_block(x, 512, 1, 1)\n",
    "    x = layers.DepthwiseConv2D(7, 1, padding='valid', use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(embedding_size)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    if num_classes:\n",
    "        x = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    model = keras.Model(inputs, x, name='MobileFaceNet')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f43df19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation\n",
    "data_dir = 'dataset'  # Path to your dataset folder\n",
    "img_height, img_width = 112, 112\n",
    "batch_size = 32\n",
    "num_classes = len(next(os.walk(data_dir))[1])\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=0.2,\n",
    "    horizontal_flip=True,\n",
    "    zoom_range=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8661bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12724 images belonging to 30 classes.\n"
     ]
    }
   ],
   "source": [
    "train_gen = datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "881d8c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save class_indices mapping\n",
    "import json\n",
    "with open('class_indices.json', 'w') as f:\n",
    "    json.dump(train_gen.class_indices, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97f82966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3165 images belonging to 30 classes.\n"
     ]
    }
   ],
   "source": [
    "val_gen = datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0fc7d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-17 13:38:14.866297: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M4\n",
      "2025-06-17 13:38:14.866329: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2025-06-17 13:38:14.866335: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2025-06-17 13:38:14.866358: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-06-17 13:38:14.866371: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "# Model Setup\n",
    "model = MobileFaceNet(input_shape=(img_height, img_width, 3), embedding_size=128, num_classes=num_classes)\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a87053c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='accuracy',      # or 'accuracy' if you want to monitor training accuracy\n",
    "    patience=5,                  # Number of epochs with no improvement after which training will be stopped\n",
    "    restore_best_weights=True,   # Restore model weights from the epoch with the best value of the monitored quantity\n",
    "    min_delta=0.001,             # Minimum change to qualify as an improvement\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc20d333",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nishantkumar/Desktop/Facial Recognition/facenet_env/lib/python3.12/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-17 13:38:21.565354: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 487ms/step - accuracy: 0.2598 - loss: 2.8701"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nishantkumar/Desktop/Facial Recognition/facenet_env/lib/python3.12/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m235s\u001b[0m 523ms/step - accuracy: 0.2600 - loss: 2.8688 - val_accuracy: 0.1422 - val_loss: 4.8353\n",
      "Epoch 2/50\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 505ms/step - accuracy: 0.4842 - loss: 1.8102 - val_accuracy: 0.4085 - val_loss: 2.4689\n",
      "Epoch 3/50\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 502ms/step - accuracy: 0.5902 - loss: 1.4308 - val_accuracy: 0.4900 - val_loss: 1.7864\n",
      "Epoch 4/50\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 510ms/step - accuracy: 0.6326 - loss: 1.2691 - val_accuracy: 0.4629 - val_loss: 2.2694\n",
      "Epoch 5/50\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 509ms/step - accuracy: 0.6912 - loss: 1.0828 - val_accuracy: 0.5602 - val_loss: 1.5272\n",
      "Epoch 6/50\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 503ms/step - accuracy: 0.7190 - loss: 0.9820 - val_accuracy: 0.5820 - val_loss: 1.4926\n",
      "Epoch 7/50\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 507ms/step - accuracy: 0.7338 - loss: 0.9154 - val_accuracy: 0.5953 - val_loss: 1.4072\n",
      "Epoch 8/50\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 504ms/step - accuracy: 0.7476 - loss: 0.8717 - val_accuracy: 0.4923 - val_loss: 1.8350\n",
      "Epoch 9/50\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 504ms/step - accuracy: 0.7414 - loss: 0.8918 - val_accuracy: 0.6891 - val_loss: 1.0846\n",
      "Epoch 10/50\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m198s\u001b[0m 497ms/step - accuracy: 0.7863 - loss: 0.7327 - val_accuracy: 0.6246 - val_loss: 1.3496\n",
      "Epoch 11/50\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m207s\u001b[0m 520ms/step - accuracy: 0.7811 - loss: 0.7479 - val_accuracy: 0.7232 - val_loss: 1.0192\n",
      "Epoch 12/50\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 511ms/step - accuracy: 0.7927 - loss: 0.7068 - val_accuracy: 0.7264 - val_loss: 1.0163\n",
      "Epoch 13/50\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m195s\u001b[0m 489ms/step - accuracy: 0.7978 - loss: 0.6900 - val_accuracy: 0.6730 - val_loss: 1.3135\n",
      "Epoch 14/50\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m194s\u001b[0m 488ms/step - accuracy: 0.8041 - loss: 0.6514 - val_accuracy: 0.7773 - val_loss: 0.8453\n",
      "Epoch 15/50\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m198s\u001b[0m 497ms/step - accuracy: 0.7917 - loss: 0.6922 - val_accuracy: 0.7191 - val_loss: 1.0432\n",
      "Epoch 16/50\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m195s\u001b[0m 490ms/step - accuracy: 0.8110 - loss: 0.6230 - val_accuracy: 0.7589 - val_loss: 0.9117\n",
      "Epoch 17/50\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m192s\u001b[0m 483ms/step - accuracy: 0.8267 - loss: 0.6009 - val_accuracy: 0.8006 - val_loss: 0.7612\n",
      "Epoch 18/50\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m195s\u001b[0m 490ms/step - accuracy: 0.8341 - loss: 0.5552 - val_accuracy: 0.5147 - val_loss: 1.9398\n",
      "Epoch 19/50\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 500ms/step - accuracy: 0.8319 - loss: 0.5613 - val_accuracy: 0.7498 - val_loss: 0.9428\n",
      "Epoch 20/50\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 503ms/step - accuracy: 0.8468 - loss: 0.5129 - val_accuracy: 0.7109 - val_loss: 1.0465\n",
      "Epoch 21/50\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m204s\u001b[0m 513ms/step - accuracy: 0.8174 - loss: 0.5890 - val_accuracy: 0.6313 - val_loss: 1.3565\n",
      "Epoch 22/50\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 504ms/step - accuracy: 0.8002 - loss: 0.6656 - val_accuracy: 0.7169 - val_loss: 1.0588\n",
      "Epoch 23/50\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 503ms/step - accuracy: 0.8445 - loss: 0.5257 - val_accuracy: 0.7444 - val_loss: 0.9407\n",
      "Epoch 24/50\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 509ms/step - accuracy: 0.8403 - loss: 0.5270 - val_accuracy: 0.6556 - val_loss: 1.2843\n",
      "Epoch 25/50\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m194s\u001b[0m 489ms/step - accuracy: 0.8607 - loss: 0.4596 - val_accuracy: 0.7744 - val_loss: 0.8410\n",
      "Epoch 26/50\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m212s\u001b[0m 532ms/step - accuracy: 0.8430 - loss: 0.5224 - val_accuracy: 0.6088 - val_loss: 1.5429\n",
      "Epoch 27/50\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 502ms/step - accuracy: 0.8428 - loss: 0.5341 - val_accuracy: 0.7896 - val_loss: 0.7934\n",
      "Epoch 28/50\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 505ms/step - accuracy: 0.8888 - loss: 0.3818 - val_accuracy: 0.7684 - val_loss: 0.8493\n",
      "Epoch 29/50\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m198s\u001b[0m 497ms/step - accuracy: 0.8854 - loss: 0.3759 - val_accuracy: 0.7406 - val_loss: 1.0109\n",
      "Epoch 30/50\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m198s\u001b[0m 497ms/step - accuracy: 0.8410 - loss: 0.5129 - val_accuracy: 0.7336 - val_loss: 0.9640\n",
      "Epoch 31/50\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 501ms/step - accuracy: 0.8567 - loss: 0.4665 - val_accuracy: 0.7349 - val_loss: 0.9656\n",
      "Epoch 32/50\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 501ms/step - accuracy: 0.8540 - loss: 0.4679 - val_accuracy: 0.6998 - val_loss: 1.1125\n",
      "Epoch 33/50\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m206s\u001b[0m 518ms/step - accuracy: 0.8316 - loss: 0.5288 - val_accuracy: 0.6863 - val_loss: 1.1833\n",
      "Epoch 34/50\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 509ms/step - accuracy: 0.8713 - loss: 0.4084 - val_accuracy: 0.7858 - val_loss: 0.8945\n",
      "Epoch 35/50\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 505ms/step - accuracy: 0.8977 - loss: 0.3290 - val_accuracy: 0.8088 - val_loss: 0.7519\n",
      "Epoch 36/50\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m198s\u001b[0m 498ms/step - accuracy: 0.8947 - loss: 0.3303 - val_accuracy: 0.8000 - val_loss: 0.7904\n",
      "Epoch 37/50\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 500ms/step - accuracy: 0.9080 - loss: 0.3006 - val_accuracy: 0.8278 - val_loss: 0.7104\n",
      "Epoch 38/50\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 503ms/step - accuracy: 0.9077 - loss: 0.2914 - val_accuracy: 0.6297 - val_loss: 1.5108\n",
      "Epoch 39/50\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 508ms/step - accuracy: 0.8469 - loss: 0.4981 - val_accuracy: 0.8111 - val_loss: 0.7657\n",
      "Epoch 40/50\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 500ms/step - accuracy: 0.8961 - loss: 0.3412 - val_accuracy: 0.7750 - val_loss: 0.9585\n",
      "Epoch 41/50\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m548s\u001b[0m 1s/step - accuracy: 0.8810 - loss: 0.3801 - val_accuracy: 0.7488 - val_loss: 1.0312\n",
      "Epoch 42/50\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m198s\u001b[0m 497ms/step - accuracy: 0.9131 - loss: 0.2896 - val_accuracy: 0.8231 - val_loss: 0.7045\n",
      "Epoch 43/50\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m207s\u001b[0m 519ms/step - accuracy: 0.9218 - loss: 0.2549 - val_accuracy: 0.8038 - val_loss: 0.8132\n",
      "Epoch 44/50\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1146s\u001b[0m 3s/step - accuracy: 0.9154 - loss: 0.2679 - val_accuracy: 0.7684 - val_loss: 0.9455\n",
      "Epoch 45/50\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m226s\u001b[0m 568ms/step - accuracy: 0.9016 - loss: 0.2940 - val_accuracy: 0.7839 - val_loss: 0.8839\n",
      "Epoch 46/50\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 505ms/step - accuracy: 0.9111 - loss: 0.2835 - val_accuracy: 0.7507 - val_loss: 0.9704\n",
      "Epoch 47/50\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m207s\u001b[0m 520ms/step - accuracy: 0.9124 - loss: 0.2642 - val_accuracy: 0.8133 - val_loss: 0.7803\n",
      "Epoch 48/50\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m684s\u001b[0m 2s/step - accuracy: 0.9083 - loss: 0.2916 - val_accuracy: 0.8265 - val_loss: 0.7060\n",
      "Epoch 49/50\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m198s\u001b[0m 498ms/step - accuracy: 0.9203 - loss: 0.2571 - val_accuracy: 0.7469 - val_loss: 1.0395\n",
      "Epoch 50/50\n",
      "\u001b[1m398/398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m198s\u001b[0m 498ms/step - accuracy: 0.9156 - loss: 0.2726 - val_accuracy: 0.7453 - val_loss: 1.1234\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x325555a30>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training\n",
    "model.fit(\n",
    "    train_gen,\n",
    "    epochs=50,\n",
    "    validation_data=val_gen\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33d31c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete. Model saved as mobilefacenet_trained.h5\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "model.save('mobilefacenet_trained.h5')\n",
    "print('Training complete. Model saved as mobilefacenet_trained.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de3b140b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved class_indices mapping to class_indices.json\n"
     ]
    }
   ],
   "source": [
    "# Save class_indices mapping for use during inference\n",
    "import json\n",
    "with open('class_indices.json', 'w') as f:\n",
    "    json.dump(train_gen.class_indices, f)\n",
    "print('Saved class_indices mapping to class_indices.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df75ec82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "facenet_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
